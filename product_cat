import streamlit as st
import pandas as pd
import base64
import io
import jieba
from collections import Counter


product_cat_dict = {
    'ç‚–': 'Stew Pot',
    "å¤šå£«ç‚‰": "Toaster",
    "å’–å•¡æœº": "Coffee Machine",
    "å’–å•¡æ¯": "Coffee Cup",
    "çƒ­æ°´å£¶": "Kettle",
    'ç‚–é”…': "Cooking Pot",
    'æ¯ç›–å­åº•åº§': 'Coaster',
    'ä¿æ¸©æ¯': 'Thermos',
    'å¥¶æ³¡': 'Milk Frother',
    'å¹³åº•ç‚’é”…':'Flat Pan',
    'çƒ§æ°´å£¶': 'Kettle',
    'çƒç…é”…': 'Stew Pot',
    'å¹³åº•é”…': 'Stew Pot',
    'ä¸ç²˜é”…': 'Stew Pot',
    'æ…æ‹Œæœº': 'Blender',
    'é”…å…·': 'Pot Set',
    'åŽ¨å¸ˆæœº': 'Stand Mixer',
    'æ–™ç†æœº': 'Stand Mixer',
    'æœ¨ç §æ¿': 'Cutting Board',
    'å’–å•¡ç£¨è±†æœº': 'Coffee Grinder',
    'å†°ç®±': 'Fridge',
    'å’–å•¡è½½æ¯': 'Travel Cup',
    'éšè¡Œæ¯': 'Travel Cup',
    'å’–å•¡é…å¥—': 'Coffee Kit',
    'æ±æœº': 'Juicer',
    'ç£¨è±†æœº': 'Cofee Grinder',
    'å’–å•¡è±†ç ”ç£¨æœº': 'Cofee Grinder',
    'å’–å•¡ç£¨ç²‰æœº': 'Cofee Grinder',
    'æ…æ‹Œæ¦¨æ±': 'Blender',
    'æ²¹çƒŸæœº': 'Range Hood',
    "ç‡ƒæ°”ç¶": "Gas Stove",
    "å®¶ç”¨ç¶å…·": "Gas Stove",
    'æŒ‚çƒ«æœº': 'Steamer',
    'é¤å…·': 'Tableware Set',
    "é£Ÿå“åŠ å·¥æœº": 'Food Processor',
    'æ‰“è›‹å™¨': 'Whisk',
    'ç ´å£': 'Blender',
    'çƒ¤é¢åŒ…æœº': 'Toaster',
    'åŽ‹é¢å™¨': 'Rolling Machine',
    'æ¦¨æ±': 'Juicer',
    'åˆ€å…·': 'Knives Set',
    'æ³¡èŒ¶': 'Kettle',
    'æ…æ‹Œå™¨': 'Blender',
    'è¿‡æ»¤å™¨': 'Filter',
    'ç£ç‚‰': 'Oven',
    'çœŸç©ºæœº': 'Vacuum Machine',
    'æ‰‹åŠ¨çœŸç©º': 'Vacuum Machine'
}

#å¹³åº•é”…

# Set page config at the beginning
st.set_page_config(page_title="CATEGORIZZAZIONE PRODOTTO", page_icon="ðŸ“‚")

# Define the function to process the uploaded Excel files
def process_files(uploaded_files, product_cat_dict):
    merged_dfs = []
    
    for file in uploaded_files:
        df = pd.read_excel(file)
        merged_df = pd.DataFrame()
        
        if 'ITEM_ID' in df.columns:
                merged_df['ITEM_ID'] = df['ITEM_ID']
        else:
                merged_df['ITEM_ID'] = df['Item ID']

        merged_df['TITLE'] = df['TITLE'] 
        
        if 'IMAGE_URL' in df.columns:
                merged_df['IMAGE_URL'] = df['IMAGE_URL']
        else:
                merged_df['IMAGE_URL'] = df['Image Url']

        if 'PRODUCT_CATEGORY' in df.columns:
                merged_df['PRODUCT_CATEGORY'] = df['PRODUCT_CATEGORY']
        else:
                merged_df['PRODUCT_CATEGORY'] = df['Product Category']
        
        
        def map_category(title):
            for keyword, category in product_cat_dict.items():
                if keyword in title:
                    return category
            return '-'
        
        merged_df['PRODUCT_CATEGORY_SUGGESTED'] = merged_df['TITLE'].apply(map_category)
        merged_dfs.append(merged_df)


        # GESTIONE ECCEZIONI
        # Function to check if both terms are present in the TITLE
        def has_breakfast_kit(title):
            return 'æ°´å£¶' in title and 'å¤šå£«ç‚‰' or 'åå¸' in title

        # Update the 'PRODUCT_CATEGORY_SUGGESTED' column based on the condition
        merged_df['PRODUCT_CATEGORY_SUGGESTED'] = merged_df.apply(
            lambda row: 'Breakfast Kit' if has_breakfast_kit(row['TITLE']) else row['PRODUCT_CATEGORY_SUGGESTED'],
            axis=1
        )
        
        # Count the number of rows with '-' in PRODUCT_CATEGORY_SUGGESTED
        no_categorization_count = (merged_df['PRODUCT_CATEGORY_SUGGESTED'] == '-').sum()
        # Total number of rows in the DataFrame
        total_rows = len(merged_df)
        # Display the message
        st.sidebar.write(f"{no_categorization_count} out of {total_rows} products have no categorization.")

        #------------------------------------------------------------------------------------
        #                                       WORDS CLOUD
        #------------------------------------------------------------------------------------



        # Filter rows with '-' in 'PRODUCT_CATEGORY_SUGGESTED'
        filtered_df = merged_df[merged_df['PRODUCT_CATEGORY_SUGGESTED'].str.contains('-')]
        # Extract the "TITLE" column from the filtered DataFrame
        title_column = filtered_df["TITLE"]
        # Combine all titles into a single string
        all_titles = " ".join(title_column)
        # Tokenize the text using jieba
        words = list(jieba.cut(all_titles))
        # Define the desired n-gram size (e.g., bigrams)
        n = 2
        # Generate n-grams
        ngrams = [tuple(words[i:i + n]) for i in range(len(words) - n + 1)]

        # Count the occurrences of each n-gram
        ngram_counts = Counter(ngrams)

        # Sort the n-grams by occurrence in descending order
        sorted_ngram_counts = sorted(ngram_counts.items(), key=lambda x: x[1], reverse=True)

        st.sidebar.write('Words Pairs most commonly used in TITLE of uncategorized listings')

        # Display the most common n-grams and their counts
        for ngram, count in sorted_ngram_counts:
            if all(len(word) >= 2 for word in ngram):  # Filter out n-grams with single-character words
                st.sidebar.write(f"{' '.join(ngram)}: {count}")

        #------------------------------------------------------------------------------------
        #                    
        #------------------------------------------------------------------------------------




    return merged_dfs

# Define the Streamlit app
st.title('Excel File Processor')

uploaded_files = st.file_uploader("Upload one or multiple XLSX files", type=["xlsx"], accept_multiple_files=True)

if uploaded_files:
    merged_dfs = process_files(uploaded_files, product_cat_dict)

    st.write("Merged DataFrame with Suggested Categories:")

    for idx, df in enumerate(merged_dfs):
        st.write(f"DataFrame {idx + 1}:")
        st.write(df)


    # Search box for filtering by 'TITLE'
    search_term = st.text_input("Search by TITLE", "")

    # Filter the DataFrame based on the search term
    if search_term:
        for idx, df in enumerate(merged_dfs):
            df_filtered = df[df['TITLE'].str.contains(search_term, case=False)]
            st.write(f"Filtered DataFrame {idx + 1}:")
            st.write(df_filtered)


    # Download the data as XLSX
    for idx, df in enumerate(merged_dfs):
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            df.to_excel(writer, sheet_name=f'DataFrame_{idx + 1}', index=False)
        b64 = base64.b64encode(output.getvalue()).decode()
        st.markdown(f"Download DataFrame {idx + 1} as XLSX: [DataFrame_{idx + 1}.xlsx](data:application/vnd.openxmlformats-officedocument.spreadsheetml.sheet;base64,{b64})", unsafe_allow_html=True)

# Run the Streamlit app
if __name__ == '__main__':
    st.set_option('deprecation.showfileUploaderEncoding', False)
    st.write('Upload one or multiple XLSX files to begin.')
